<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>use.perl.org journal of jdavidb: Multilayer neural network implementation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="/static/css/bootstrap.css" rel="stylesheet">
    <style>
      body {
        padding-top: 60px; /* 60px to make the container go all the way to the
bottom of the topbar */
      }
    </style>
    <link href="/static/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script
src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/static/ico/favicon.ico">
  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse"
data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/"><img src="/static/img/slashhead.png"/></a>
          <div class="nav-collapse">
            <ul class="nav">
              <li><a href="/">Home</a></li>
              <li><a href="/about/">About</a></li>
              <li><a href="/authors/">Authors</a></li>
              <li><a href="/journals/">Journals</a></li>
              <li><a href="/stories/">Stories</a></li>
            </ul>
            <p class="navbar-text">All the Perl that's Practical to Extract and Report</p>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>



<div class="container">

<div class="row">

<div class="span4">
<h1>Multilayer neural network implementation</h1>
<h2><a href="/user/jdavidb/">jdavidb</a> on 2002-04-29T06:29:01</h2>
</div> <!-- /span4 -->


<div class="span8">
<p><p>I'm working on my last code of the semester!  Unfortunately that means I'm burning a day off tomorrow so I can stay up all night tonight.  After my trip to Ukraine earlier this month I'd kind of gotten used to eight hours of sleep every day.  I may make a habit of it this summer.</p>

<p>The good news is, I get to do this project in Perl.  It's for Neural Networks class.  If you've never done neural networks before you might have a preconception like I used to that everything is probably best implemented as a collection of instances of a Neuron class.  Actually, everything is best implemented as a ton of matrix math.  Our professor has been encouraging us to use matlab.  I've been drinking FSF kool-aid for a couple of years now, and matlab isn't (yet) available for Debian MacPPC GNU/Linux (go figure), so I've spent most of this semester with Octave, a less-capable but very adequate alternative.  However, I started the class by trying to use PDL, and with this project, I've returned to it.</p>

<p>So now I have a complete pure-Perl implementation of a multilayer neural network class complete with backpropagation training algorithm.  I've even tested it with an example from the book and found it to work.  I am certain that during this night, however, my mileage will vary.  Specifically, I've verified all the math through one training iteration that my textbook goes over in great detail, and then ran through enough training iterations on the same single training set to verify that the thing really did converge.</p>

<p>I'll pause here to mention that this kind of thing gives me an ecstatic creative thrill.</p>

<p>For the rest of the project, I have to do something useful with it.</p>

<p>Of course, this code will be free software.  I'll try to get it into CPAN some time in the next three months, but if you're reading this in two years and don't see it, post here asking for it.  I never throw anything away, so I'm sure I'll still have it.</p>

<p>So, what do I call this monstrosity?  Currently it's just Network.pm.  There seem to be two different neural network implementations in CPAN (and several pointers to authors who apparently never finished or never released).  One of these seems to tie in with another library.  I didn't look at the other one yet, but if it doesn't use PDL it's probably way different from mine.  (If it does, I probably just reinvented the wheel.)  So ... AI::NeuralNetwork::PDL?  AI::NeuralNetwork::Multilayered?  Ah, I'll worry about it some day when I've had enough sleep.</p>

<p>No, there's no test suite.  I'm not an XP guy, at least not this year.</p>

<p>BTW, the class is general enough you can have any number of layers, any number of neurons in each layer, and any type of activation function for each layer, as long as you provide coderefs for each activation function <i>and</i> its derivative.  All neurons in a single layer have to have the same activation function (but I think that's part of the definition, anyway).</p></p>


<hr/>



<h2>talk about deja-vu</h2>
<h3><a href="/user/tinman/">tinman</a> on 2002-05-06T08:00:34</h3>
<p>Had the same assignment (graduated in 2000) in my final year... our team members were for VB, Java and Perl (Perl was just me).. but at that time, the intricacies of PDL were a bit too much for me, so I abandoned the idea of Perl and we ended up doing it in VB
</p><p>As it turned out, Perl benched slightly slower than Java and about the same as VB for a 3 layer back propagation network.. We had loads of fun though... the assignment required that we build a character recognition system (any alpha numeric char is valid), and additionally, we needed to build a calculator (scribble digits on a scratchpad and it adds, multiplies etc etc)...
</p><p>Having been through "network training hell", I can tell you, the coding is basically about 1% of the effort required here<nobr> <wbr></nobr>:o) if you have an open ended problem with no "ideal" configuration already given, you need to try out an endless series of algorithms and layer permutations to get it going right.. We tried Hebbian, back prop and *mumble* several other thingies which I've allowed myself to forget<nobr> <wbr></nobr>;o)
</p><p>Perhaps a search on CPAN might help, though ? I *know* I saw some ANN related Perl modules there, no idea about maintenance.. try <a href="http://search.cpan.org/search?mode=module&amp;query=neural" title="cpan.org">this search</a cpan.org> if you haven't already, might start you off at square 2 or 3 as opposed to 1<nobr> <wbr></nobr>;o)</p>





</div> <!-- /span8 -->

</div> <!-- row -->
</div> <!-- /container -->



    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

  </body>
</html>

