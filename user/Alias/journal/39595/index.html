<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>use.perl.org journal of Alias: Improvement Needed - Perl needs better benchmarking</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="/static/css/bootstrap.css" rel="stylesheet">
    <style>
      body {
        padding-top: 60px; /* 60px to make the container go all the way to the
bottom of the topbar */
      }
    </style>
    <link href="/static/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script
src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/static/ico/favicon.ico">
  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse"
data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/"><img src="/static/img/slashhead.png"/></a>
          <div class="nav-collapse">
            <ul class="nav">
              <li><a href="/">Home</a></li>
              <li><a href="/about/">About</a></li>
              <li><a href="/authors/">Authors</a></li>
              <li><a href="/journals/">Journals</a></li>
              <li><a href="/stories/">Stories</a></li>
            </ul>
            <p class="navbar-text">All the Perl that's Practical to Extract and Report</p>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>



<div class="container">

<div class="row">

<div class="span4">
<h1>Improvement Needed - Perl needs better benchmarking</h1>
<h2><a href="/user/Alias/">Alias</a> on 2009-09-07T16:41:36</h2>
</div> <!-- /span4 -->


<div class="span8">
<p>My previous post on Moose contained a fairly typical benchmark, used in a fairly typical way.<br/><br/><pre><br/><br/># Object::Tiny vs Object::Tiny::XS vs Moose::Tiny
# Note that I had to do __PACKAGE__->meta->make_immutable to 
# the Moose::Tiny class, because it doesn't to so itself.<br/><br/># Based on the following Object::Tiny class.
use Object::Tiny qw{
    one
    two
    three
    four
    five
};<br/><br/>D:\DOCUME~1\ADAM~1.KEN\Desktop>perl moosemark.pl
                Rate moose_all    xs_all tiny_all moose_none tiny_none   xs_none
moose_all    82795/s        --      -54%     -54%       -72%      -90%      -92%
xs_all      178763/s      116%        --      -1%       -39%      -79%      -83%
tiny_all    179759/s      117%        1%       --       -39%      -79%      -83%
moose_none  294985/s      256%       65%      64%         --      -65%      -72%
tiny_none   842460/s      918%      371%     369%       186%        --      -21%
xs_none    1066098/s     1188%      496%     493%       261%       27%        --
            (warning: too few iterations for a reliable count)
               Rate moose_get  tiny_get    xs_get
moose_get 1730104/s        --      -11%      -32%
tiny_get  1937984/s       12%        --      -24%
xs_get    2557545/s       48%       32%        --<br/><br/></pre><br/><br/>Now, I'm not showing this again to rub in that Moose is slower than Class::XSAccessor, because even as recently as today there XS acceleration code about to land and start making up the difference.<br/><br/>The problem here is that this benchmark isn't tangible.<br/><br/>You can't reuse it. You can't identify it. You can't package it or upload it to the CPAN. The closest you can get is to just randomly drop the raw script somewhere and hope for the best.<br/><br/>And god forbid you want a benchmark that will try to establish theta or O(), a benchmark that run itself across a set or a range of n, and produce a chart, or auto-discover O(). Or vary over multiple variables.<br/><br/>A benchmark with encoded results, in the same way that TAP is encoding of test results. A benchmark I can save in a /bm/ directory, the same way that I can save my tests in a /t/ directory.<br/><br/>So where is it? Where is my Class::Benchmark? My Benchmark::Archive that uploads to my Benchmark::Smolder?<br/><br/>Dear LazyWeb.<br/><br/>Can you make us a benchmark stack that is as good as our testing stack?<br/><br/>Please...?</p>


<hr/>



<h2>I have the answer</h2>
<h3><a href="/user/Burak/">Burak</a> on 2009-09-07T19:19:08</h3>
<p>42!</p>



<h2>benchmarks are tests</h2>
<h3><a href="/user/daxim/">daxim</a> on 2009-09-07T20:27:31</h3>
<blockquote><div><p>You can't package it or upload it to the CPAN.</p></div></blockquote><p>

<a href="http://search.cpan.org/dist/App-Benchmark-Accessors/" title="cpan.org">http://search.cpan.org/dist/App-Benchmark-Accessors/</a cpan.org>
</p><p>
<a href="http://hanekomu.at/blog/tags/benchmarks" title="hanekomu.at">http://hanekomu.at/blog/tags/benchmarks</a hanekomu.at></p>



<blockquote>

<h2>Re:benchmarks are tests</h2>
<h3><a href="/user/Alias/">Alias</a> on 2009-09-08T01:06:24</h3>
<p>The former embeds the benchmarks into the test suite, they aren't installed.</p>





</blockquote>


<h2>DDT &amp;amp; Benchmarking</h2>
<h3><a href="/user/Hercynium/">Hercynium</a> on 2009-09-08T02:27:44</h3>
I wonder if your post today was inspired by what I wrote in <a href="http://use.perl.org/comments.pl?sid=43745&amp;cid=70497" title="perl.org">reply</a perl.org> to your last post or we're just on the same wavelength with this.<nobr> <wbr></nobr>:)<br>
<br>
At YAPC::NA this year I did a lightning talk on a web-testing framework I built for a client*. The main topic was just that the tests were all data-driven, and because of that I gained a pile of great functionality, not least of which was the ability to turn my test suite into a benchmarking suite seamlessly.<br>
<br>
By running via prove or directly, each script was a set of unit tests. By running under a utility I called "bench", those same test scripts became benchmarks (unless marked as not suitable in the test data structure)<br>
<br>
*Funny story - schwern told me "Congratulations, you've just re-invented Selenium!" Lo and behold I looked at Selenium and showed it to the client and they are *delighted* that I threw away a month's work!<nobr> <wbr></nobr>;-)



<blockquote>

<h2>Re:DDT &amp;amp; Benchmarking</h2>
<h3><a href="/user/Alias/">Alias</a> on 2009-09-08T03:22:03</h3>
<p>It's mostly because of some issues at work.</p><p>We've got ourselves a shiny new load testing environment, which is useful for catching major performance regressions.</p><p>But it doesn't easily catch slowdowns in strange and unusual places. Stuff beyond just the "scrape the website" tests, things that only run in the backend and do a lot of heavy lifting.</p><p>What we'd like is a way to have a benchmark suite that sits in the project, runs nightly with the smoke tests, and logs to a database so that we can do performance trend-spotting on specific functions or processes.</p><p>Web performance testing is important too, but more complex. The one we need the most is a way to test more tightly than that.</p>



<h2>Re:DDT &amp;amp; Benchmarking</h2>
<h3><a href="/user/tgape/">tgape</a> on 2009-09-09T02:49:39</h3>
<p>I think Alias addressed most of my would-be response, so I'll limit my comments to:</p><p>All of the efforts I've personally seen to turn unit tests into benchmarks ended up being more of a benchmark of perl's compiler than anything else.&nbsp; Unit tests tend to focus on tiny portions of the overall code, and test just them; they tend to not do unnecessary looping.&nbsp; As such, the naive 'turn them into benchmarks' puts the loop around them, and unfortunately either re-evals them, or re-forks and evals them.</p><p>One would be better off turning ones functional tests into benchmarks - on average (at least for me) they test a larger portion of the code per script, but tend to not be as complex, proportionately speaking.&nbsp; Some of my functional tests have even been distilled down to the point of "Directory A contains a bunch of input files, directory B contains files with the same names which are their corresponding output files.&nbsp; See if they generate the right output."&nbsp; When that's running against a persistent process, it can be looped without too much worry.</p><p>Of course, the real benchmark is probably more like, "Directory A contains a bunch of input files; each one corresponds to a real session with its data munged to eliminate personal details.&nbsp; Each session file resets its state at the end.&nbsp; Directory B contains the expected output for each of these sessions.&nbsp; Run them all at the same time, repeatedly."</p>



<blockquote>

<h2>Re:DDT &amp;amp; Benchmarking</h2>
<h3><a href="/user/Hercynium/">Hercynium</a> on 2009-09-09T03:10:31</h3>
<p>Indeed, in this case the test harness *is* testing on a more functional/integration level. As we refactor the code that runs the client's site, we want to ensure that we neither break any part of the site nor slow it down.</p><p>Still, using this type of technique has made me wonder if there is a good way of encouraging benchmarks of perl modules by piggy-backing on the tests.</p><p>An example of this can be found in the test suite for Sort::Maker. <a href="http://search.cpan.org/dist/Sort-Maker/" title="cpan.org">http://search.cpan.org/dist/Sort-Maker/</a cpan.org> (It was actually Uri's advice to look at that module's tests that inspired the design of my website DDT code)</p><p>I do understand that benchmarking typically needs much more "context" than unit tests, but a test harness with an API that enables/encourages 'performance comparisons' with other code that does the same thing could be a good springboard to get people providing benchmarkable code/tests in their CPAN dists.</p>



<h2>Re:DDT &amp;amp; Benchmarking</h2>
<h3><a href="/user/Alias/">Alias</a> on 2009-09-09T04:41:21</h3>
<p>One of the other problems we have is that because our system is enormous, with many links to external systems and pre-compiled caches, it takes two to three orders of magnitude more time to set up for a functional test than it does to run the actual tests.</p><p>I think any sufficiently robust solution is going to need a way to factor that out, and this is a problem for the test script overloading approach.</p><p>The test scripts would need a way to isolate the code you are interested in from the code you are running to set up for the actual test code.</p>





</blockquote>

</blockquote>


<h2>Spec's welcome</h2>
<h3><a href="/user/notbenh/">notbenh</a> on 2009-09-08T19:29:29</h3>
<p>a few members of pdx.pm (including me) are working on a benchmarking framework with the intent of being able to build nice charts and graphs to show rakudo getting faster and to compare it to perl5. We're using the euler problem set just as an example for now but if you have specific specs that you would like us to hit I would love to hear them.</p><p><a href="http://github.com/notbenh/euler_bench/tree/master" title="github.com">http://github.com/notbenh/euler_bench/tree/master</a github.com></p>



<blockquote>

<h2>Re:Spec's welcome</h2>
<h3><a href="/user/Alias/">Alias</a> on 2009-09-09T03:22:38</h3>
<p>Apart from a 120 line<nobr> <wbr></nobr>.pm file with very little in it, I'm not seeing much there in the way of frameworky'ness.</p><p>Do you have a more accurate URL?</p>





</div> <!-- /span8 -->

</div> <!-- row -->
</div> <!-- /container -->



    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

  </body>
</html>

