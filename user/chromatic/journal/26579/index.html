<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>use.perl.org journal of chromatic: Secrets of Contextual Analysis</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="/static/css/bootstrap.css" rel="stylesheet">
    <style>
      body {
        padding-top: 60px; /* 60px to make the container go all the way to the
bottom of the topbar */
      }
    </style>
    <link href="/static/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script
src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/static/ico/favicon.ico">
  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse"
data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/"><img src="/static/img/slashhead.png"/></a>
          <div class="nav-collapse">
            <ul class="nav">
              <li><a href="/">Home</a></li>
              <li><a href="/about/">About</a></li>
              <li><a href="/authors/">Authors</a></li>
              <li><a href="/journals/">Journals</a></li>
              <li><a href="/stories/">Stories</a></li>
            </ul>
            <p class="navbar-text">All the Perl that's Practical to Extract and Report</p>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>



<div class="container">

<div class="row">

<div class="span4">
<h1>Secrets of Contextual Analysis</h1>
<h2><a href="/user/chromatic/">chromatic</a> on 2005-09-02T22:12:58</h2>
</div> <!-- /span4 -->


<div class="span8">
<p><p>I'm analyzing the content of some documents in order to find potential correlations between them.  Breaking each document into individual words, stemming those words, and throwing out the stopwords gave me some 18,000 unique words from a 600-document corpus, with over 40% of words appearing only once in the corpus and almost 80% of the words appearing fewer than ten times.</p>

<p>I knew my existing list of stop words was insufficient, but I really don't want to pick out the top 1000 or 2000 useful words from a list of 18,000, especially because this is a test corpus of perhaps 7% of the actual corpus.</p>

<p>Now I start to wonder if some of the lexical analysis modules would be useful in picking out only the nouns (unstemmed) and verbs (stemmed) from a document, rather than taking all of the words of a document as significant.  The correlation algorithm appears sound, but if I can throw out lots of irrelevant data, I can improve the performance and utility of the application.</p>

<p>Any thoughts?</p></p>


<hr/>



<h2>Breaking it down by word?</h2>
<h3><a href="/user/Ovid/">Ovid</a> on 2005-09-02T22:40:33</h3>
<p>What sort of correlations are you looking for?  I was focusing on <a href="http://perlmonks.org/index.pl?displaytype=display;node_id=464805" title="perlmonks.org">detecting plagiarism</a perlmonks.org> at one point and found that breaking things down by sentence was more useful.  As I don't know what you're trying to do, I've no idea if that link will prove useful.</p>



<blockquote>

<h2>Re:Breaking it down by word?</h2>
<h3><a href="/user/chromatic/">chromatic</a> on 2005-09-02T23:39:12</h3>
<p>Detecting plagiarism is much more specific than this problem.  I want to be able to analyze a document and suggest a handful of other documents that, from their intertextual context at least, appear to discuss similar things.  For example, a tutorial about creating homemade pizza dough is probably not very similar to a journal entry about linguistic analysis, but probably is similar to an article discussing  different types of pizza ovens.</p>

<p>I'm trying to answer the question "Do the relevant topics of these documents overlap?", not necessarily "Do these documents share a common ancestry?"</p>



<blockquote>

<h2>Re:Breaking it down by word?</h2>
<h3><a href="/user/Ovid/">Ovid</a> on 2005-09-03T00:04:22</h3>
<p>I see.  That makes sense.  Perhaps a heuristic approach is best as there are few algorithms likely to realize that "July heat wave" and "dog days of summer" might be related, though when the text is long enough idiomatic expressions are likely to come out in the wash.</p>

<p>My initial thought would be to try to score words in documents.  Take the words that appear the most frequently and somehow correlate their frequency in the document by their <em>infrequency</em> in the language.  Thus, the least common words which appear the the most often would generate a higher score.  You could do a first-pass straight comparison or try to go further with synonyms (or even antonyms).  You could havee even more fun by trying to account for misspellings<nobr> <wbr></nobr>:)</p>



<blockquote>

<h2>Re:Breaking it down by word?</h2>
<h3><a href="/user/tinman/">tinman</a> on 2005-09-03T16:27:16</h3>
<p>From WordNet::Similarity.. there is an algorithm in there by Lesk (from a paper in 1986 which Citeseer and Scholar don't seem to find a reference for); which uses something they call gloss overlap.</p>

<p>What Lesk basically does is what Ovid suggests, except the calculation is performed for Wordnet glossary definitions and not for entire documents.</p>



<h2>Re:Breaking it down by word?</h2>
<h3><a href="/user/saorge/">saorge</a> on 2005-09-13T16:20:58</h3>
Your initial thought is called Zipf's law in information retrieval.





</blockquote>


<h2>Re:Breaking it down by word?</h2>
<h3><a href="/user/ziggy/">ziggy</a> on 2005-09-04T02:23:43</h3>
So, something similar to Amazon's "statistically improbable phrases"?





</blockquote>

</blockquote>


<h2>LSI</h2>
<h3><a href="/user/jesse/">jesse</a> on 2005-09-03T02:47:56</h3>
Perhaps Latent Semantic Analysis / Contextual Network Graphs are the hammer you're looking for:<br/><br/><a href="http://www.perl.com/lpt/a/2003/02/19/engine.html" title="perl.com">http://www.perl.com/lpt/a/2003/02/19/engine.html</a perl.com>
<a href="http://www.nitle.org/lsi/intro/" title="nitle.org">http://www.nitle.org/lsi/intro/</a nitle.org><br/><br/>I _know_ there was code on CPAN to do this in 2003, but I can't find it.



<blockquote>

<h2>Re:LSI</h2>
<h3><a href="/user/sky/">sky</a> on 2005-09-03T08:40:31</h3>
Search::ContextGraph





</blockquote>


<h2>a couple of suggestions</h2>
<h3><a href="/user/tinman/">tinman</a> on 2005-09-03T16:22:04</h3>
<p>You could use Ted Pedersen's Wordnet::Similarity modules. This attaches a numerical value to any two words and can help you identify which words are related, and how closely. I prefer jcn (the Jiang Conrath method) myself, but there are 10 different techniques on offer.</p>

<p>Also, would it not make sense to use a POS (Part of Speech) tagger before you break down stop words and so on ? I can't recommend a Perl based POS tagger offhand, since most of my work in this area is done in Java... but I'm pretty sure they must exist. Do a POS tag (to find out noun, verb, adjective contexts for individual sentences), <i>then</i> do what you're doing now. This way, you would get both the word + the part of speech tag.

For example, <i>like</i> has at least seven different contexts in which it may be used... which range from verb to adjective. <i>fling</i> could be either a noun or a verb.. and so on. Might give you a bit more granularity to work with..</p>



<h2>Many solutions</h2>
<h3><a href="/user/saorge/">saorge</a> on 2005-09-13T16:36:04</h3>
There are many solutions in information retrieval. You can compute a cosine measure between a interesting document and each document of the corpus.
You can too train a bayesian network to categorize your documents.<br/><br/>Some references :
- Information Retrieval / C. J. van RIJSBERGEN . - <a href="http://www.dcs.gla.ac.uk/Keith/Preface.html" title="gla.ac.uk">http://www.dcs.gla.ac.uk/Keith/Preface.html</a gla.ac.uk> (and specificaly the 3rd chapter : <a href="http://www.dcs.gla.ac.uk/Keith/Chapter.3/Ch.3.html" title="gla.ac.uk">http://www.dcs.gla.ac.uk/Keith/Chapter.3/Ch.3.html</a gla.ac.uk>)
- Bayesian Analysis For RSS Reading / Simon Cozens, in The Perl Journal, March 2004
- Building a Vector Space Search Engine in Perl / Maciej Ceglowski, in Perl.com, 19/02/2003 . - <a href="http://www.perl.com/pub/a/2003/02/19/engine.html" title="perl.com">http://www.perl.com/pub/a/2003/02/19/engine.html</a perl.com><br/><br/>Hope this help



<blockquote>

<h2>Re:Many solutions</h2>
<h3><a href="/user/saorge/">saorge</a> on 2005-09-13T16:38:40</h3>
Sorry for the poor layout of my comment, I've forgot to select Plain Old Text or to preview my comment.





</div> <!-- /span8 -->

</div> <!-- row -->
</div> <!-- /container -->



    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

  </body>
</html>

